# 按自己的理解记忆

对并发的理解随着工作内容的变化也有一些新的看法，算是一直在沉淀。这里结合工作经历来记录自己思考的变化。在早期的安防领域工作经验中，系统讲究的是实时性和可靠性，在给定时间范围内必须处理完某些时间，这是由国家的行业标准规定的。系统虽然也不小，主机控制节点数过万，这相当于业务同时在线人数过万。要想想主机是主频800MHZ的单核芯片加上256M的内存，这跟常见的业务系统主机的强大多核CPU和动辄几十G的内存是两个完全不同的环境。这个精致的小型系统中，有几点设计是值得提及的：

1. 以message based的单IO线程异步接收消息，这在现代化的编程中广泛使用，。
2. 以active queue为基础的thread based的多线程应用层通过队列通信，能够通过添加新的Access point来扩展应用功能。这跟现代化编程中的微服务概念相似，尽量避免共享内存和探知其他平级服务的内部知识。
3. 以抢占式线程调度，QoS的网络通信，高低优先级两条并行的回路来确保实时性，在每一个环节都确保高优先级事件优先得到处理。这在现代化的编程中基本是看不到的，这个设计要付出的代价太高昂。普通的民用业务系统不需要这样强的实时性。
4. 可靠性基本是通过冗余的硬件来保证，也是主从切换这一套做法。
5. 系统使用C++语言，没有GC的STW。所有内存都在启动期间申请并池化，启动完成后没有任何动态内存。这中极其严格的做法牺牲了开发效率但极大程度的提升了系统的稳定性，这也与面向普通业务的编程是不符的。不夸张的说，很多人不一定清楚没有动态内存意味着什么。

来到互联网行业，并发的难度确实被放大了很多。多核，多机器，流量高峰等将整个战场的等级提升了几个数量级。对比来看也就网络和IO这块的模型没有什么变化，这是因为协议栈和异步多路复用实在太稳定，以至于10年时间里还是差不多的样子。

## 处理并发的通用思路

高并发的能力可以简单理解为系统在一段时间里面的吞吐量。这个一段时间的概念是很重要的，没有任何系统能够稳定持续处理超过系统的吞吐量的任务，这个就算是以100%的效率来工作也是无济于事的。

1. 提升单机性能，就是换更牛逼的设备。这个解决办法最简单好用，其最大的问题就是收益与投资不成比例。除开部分头部公司的业务，小型企业的业务量如果稳定而且不会有太多增长完全可以这么做，效果决立竿见影，根本不需要去折腾其他方案。对于中型业务量的公司这招以后可能没那么好用了，因为硬件的提升空间不像以前那么大了。
2. 使用分布式的系统，用多个便宜的主机来水平扩展吞吐量。这个方式应该是大部分头部公司的选择，毕竟成本低。流量高峰只是短期而且在特定时刻可以预测的，例如各种节假日和活动都是可以按照你是数据来预估的，这样用不完的量就能以多租户的方式租出去收取租金。拥有大量主机的公司基本都有云业务，一方面自己要用，而且还可以在闲时找人分摊成本。分布式那就有得折腾了，这个打算用一篇单独的文章来讲。
3. 缓存和，是以不稳定的服务质量来换取处理时间。CPU跟内存之间有缓存，内存跟硬盘之间也有缓存，一环套一环来提高低速部件的使用效率。缓存没有命中的时候就慢一点，缓存命中了那就很快。本质上还是系统各个组件之间的匹配程度问题。
4. 异步服务基本上是普通民用业务的首选策略，以可以接受的服务质量来处理事情。大部分场景下没有人会在乎你是10毫秒还是100毫秒搞定用户的一件事情，甚至于大部分场景里都是秒级别的。这样系统就可以收集一大波请求，集中时间批量处理来提高运转效率。异步基本上就是各种队列和回调，先收着积压起来处理好了再通知。但我个人觉得这里异步的吞吐量有点伪命题的意思，如果系统积压的量超出某个限定后告诉用户失败其实这可能不能算作吞吐量的。比如秒杀100个低价商品，就算队列积压了1000条请求其实只执行了100额商品的买卖业务逻辑，后面的全部都直接打回去了。

就拿去宜家家居买东西来做比喻描述上面几个方法（上个月刚去佛山旗舰店）：
scale up是把店面建成旗舰店，面积大展区多收银台多服务区人手多。边逛边体验，喜欢的就丢到购物车，逛完去收银台买单去服务台开发票走人，无论哪个环节顾客几乎都不用等。
scale out 就是一家店不够就开多几家，把顾客分流到不同的店里去，尽量让各个分店的体验接近旗舰店。
cache 缓存的理解跟文章不一样，缓存的基础是系统有热点，有少数东西的使用率特别高。对宜家的例子来说缓存就像top10的热卖商品的专用摊位，整个店里有几个关键的路口都有摆上。这些销量占比非常大的产品顾客很容易扔进购物车，而不需要像购买普通商品那样跑到哪层楼哪个区哪个货架才能拿到，这加速了顾客的购买过程。
asyn 异步这个话题有点大。理解估计也有不少偏差，明确的说目前服务器上用的最好的linux的epoll是同步的，可以选择阻塞或者不阻塞。真正的异步IO实现有linux的aio（出来的晚，目前还不好用）和windows的IOCP。对于宜家来讲开纸质发票就是同步的，一堆人排队把小票给服务人员后在服务台等机器出发票。而开电子发票是异步的，顾客直接回家，想开票的时候扫描小票上的二维码填写发票信息，等会电子发票弄好了就发到邮箱了。

不管手段如何，适合公司当前业务量的系统才是最重要的。随着业务的成长赚到钱了，一定有重构系统的机会（资金和人才以及想赚更多的钱）。如果都按照百万、千万并发来设计系统，电商一律向淘宝看齐，IM全都学习微信和QQ，你没有那么牛逼的工程师和业务量，根本驾驭不了复杂的系统。

## 架构分层

不要看到分层就想到MVC之类的概念，那是一个狭义的例子。说大一点有应用层、控制层、和数据处理。再看看领域驱动设计这本书就会有更多的体会，牛逼的系统应该是深刻理解业务的架构师们锤炼出来的。复杂的系统基本上都要组件化，然后不同的组件也有不同的层次，上层的工作依赖于下层。像协议栈短小精悍，防火主机庞大系统的的IO以及Application层都是为了降低系统的复杂度和尽量保持可复用的组件来适应变化。不足的地方是有一定的复杂性，一个需求可能涉及到全部层次的模块的小范围更新，熟悉系统的人可能做起来比较容易，新进的成员需要一段时间来理解分层带来的复杂性。

## 指标

高并发系统设计的三大目标：高性能、高可用、可扩展。性能反应了系统的使用体验，就是系统的客户的第一感觉。你自己开车，车加速，操控如何都是驾驶员能够感受的到的。可用就是车子正常保养，几乎或者很少掉链子，不会把你丢在路上。扩展就是平时就你一个人开车上下班，但真到周末了一大家子人出去的时候还能够坐得舒服，最好还能加个车顶行李箱之类轻松搞定一家子的行李。

### 性能

性能是够用就好，不要盲目的折腾。一般来讲肯定是某个地方出现瓶颈限制了性能，最好针对瓶颈做优化满足量化的性能指标即可。这是一个长期的过程，还是问题或者说是风险驱动的。就给定吞吐量的情况下谈指标，平均值不敏感，最大值又过于敏感，分位值反应的要略清楚一些。在请求完全是并行的情况下，增加服务的进程可以获得良好的性能提升，但这也有个限度就是多进程消耗了过多的资源后造成系统繁忙反而会导致整体性能下降，这个要依靠性能测试。另外一种比较难的做法就是减少任务的单词执行时间，CPU密集型的就要靠算法了，IO密集型要靠各种缓存和池化技术以及查看绨IO耗时过久的原因。

有个老哥讲的非常好，简直不能更加同意了：业务价值->承载高并发->性能优化。一切的前提是业务价值需要。如果没有足够的价值，那么可读性才是第一，性能在需要的地方是no.1，但不需要的地方可能就是倒数第一稞。当下技术框架出来的软件差不到哪去，没有这种及时响应诉求的地方，削峰下慢慢跑就是了。（工作需要，常在缺少价值的地方着手性能优化，让我对这种就为个数字的操作很反感。要知道，异步，并发编程，逻辑缓存，算法真的会加剧系统的复杂度，得不偿失。如果没那个价值，简单才是王道）

在系统慢的时候目光要聚焦在降低响应时间这块，这个对于用户体验来说有最直观的感受。

1. 采用非阻塞的rpc调用（高效的远端请求模式，采用容器的覆盖网络我认为也算）。
2. 将计算密集和io密集的的逻辑分割开，单独线程池，调整线程比例压榨单机性能（或者说找拐点）。
3. 做缓存，io耗时的缓存和计算耗时的缓存（多级缓存，数据压缩降低带宽）。
4. 采用享元模式，用好对象池和本地线程空间，尽量减少对象创建与销毁的开销，提高复用。
5. 业务拆分，像状态变化后的外部系统通知，业务监控，es或solr等副本数据同步等操作，无需在主流程中做的事都拆掉。走canal监听表数据变化，推mq保最终一致的方式从业务项目完全解偶出来。
6. fork_join，分而治之的处理大任务。并发编程，采用多线程并行的方式处理业务。（规避伪共享，减小锁力度，采用合适的锁）。
7. 数据库配置优化，查询优化。（存储优化比较头疼，毕竟不按业务拆单点跑不掉，单点性能就要命。基本只能内存库先行，后台同步数据做持久。然后内存库多副本，自修复，保留一系列自修复失败的修复手段）

### 可用

可用性的主要指标如下：
MTBF（Mean Time Between Failure）是平均故障间隔的意思，代表两次故障的间隔时间，也就是系统正常运转的平均时间。这个时间越长，系统稳定性越高。
MTTR（Mean Time To Repair）表示故障的平均恢复时间，也可以理解为平均故障时间。这个值越小，故障对于用户的影响越小。
Availability = MTBF / (MTBF + MTTR)，降低故障恢复时间（减小分母），增加系统正常运行时间（增大分子）就能提高可用性。

大部分的可用性是通过冗余来实现，有同级别节点和主备节点有两种表现形式。同级别的节点同时承担读写流量，不保存状态，挂掉了的量会被均衡到其他节点再试。主备中主可以读和写，备节点可以是热备（只可以读，满足一定条件后可以成为主节点），也可以是冷备（只用来备份数据，不对外提供服务）。以前做的防火主机以主备的方式提供了两个层级的冗余，中心主机是冷备，在发现主节点挂掉的情况下回重启系统接管所有设备。设备主机是热备（持有所有数据，但平时也不提供服务），可以不断电接管一条总线上的所有设备。

以前以商业机器和商业软件为主的环境中，可用性都是很高的。毕竟商业硬件和付费的软件都是业界顶级的人员花费大力气做出来的，达不到宣称的指标出了问题那是要給用户大笔赔偿金的。除开金融和一些特定的领域（能够付出足够的代价追求极致的可用性），目前主流方案基本上是用便宜的机器搭建分布式的系统以冗余的方式提供可用性。机器本身稳定性一般，多个机器之间的网络连接太多也不够稳定，在硬件基础上就很容易出问题。而分布式的开发的复杂度又使得软件的质量不够高，出错也是常事（明确的错误码还好，延时才是杀手。一个长链路上的任何一个环节的延时都有可能被放大到系统的上游，而且这个环节持有的资源会加重系统的整体负担）。越大的集群越是容易碰到各种问题。可以把冗余理解成锅盖比锅多一点点，一旦出问题了可以用多于的锅盖去覆盖缺口。如果同一时间点问题多了，锅盖还是不够用的。

前面都是讲正面搞定问题的方法，可用其实还有有损的手段，算是主动取舍吧。比如控制超时，这样系统忙的时候肯定有些用户容易被超时抛弃，进而减少总的请求量。直接点限流就是按照既定策略放请求进来，进不来的就直接告诉你失败了。还有一种叫做服务降级的做法，如果系统真的是很忙了那就把一些消耗资源但又不是那么重要的业务或者流程先停掉，留下资源的同时也有可能降低了一些链路的长度。

不管采用什么手段，系统和各种功能都是工程师写出来的，没有线上环境的考验很难说有没有问题。上线前没有类似的环境做验证的话就很难发现一些问题。用灰度批量逐步发布版本，再观察各种指标和日志，尽量早的发现潜在的问题。高级点的有钱有闲的可以考虑Netflix的破环工具，搭建一套镜像系统搞点随机破坏看看结果。

### 扩展

扩展是要花钱的，不到时机是没有理由扩展的。平时系统留出50%的余量即可，在有计划的运营和推广活动中把这个扩展到一倍，用完了再还回去。比较怕的是那种突发流量，就是特定新闻事件导致一段不短的时间内的持续流量高峰。这种只能靠临时加大批量的机器了，平时没有演练过，慌忙火急的加机器是不是能够扛得住流量那是个未知数，如果搞不定就变成事故了，要等新闻事件的效应过去了系统才能够恢复正常。系统中总有些瓶颈跟机器数量是没有关系的，比如单数据库上的写请求，gate的带宽等。扩展就是要靠提前识别出系统各个部分正交的属性，对于可能成为瓶颈的地方提前拆出来成为独立的部分。拆分具体可以落实到存储（数据库）和业务（微服务的一套）。业务的接口可以设定权重，比如自己人，第三方，核心或者可以舍弃等等依靠可能造成的损失来梳理，真有问题需要取舍的时候可以立刻动手。

## 技术

要了达到一系列的各种指标，有许多通用的技术。这里还是只能用自己的理解来记录，有篇幅的时候就加一点生活中的例子。

### 池化

典型的以空间换时间的游戏，但要注意给上个保险，就是空间不能无限制使用，最好有个监控机制。这个以前的防火主机开发中那是一个溜，对存储这块的池化是应用到极致了。内存池有block pool给各种对象使用，byte pool给一些不常见的临时申请使用。基本上做到了主要对象（某个类的实例）的负载监控监控和预警，倾向于把系统最忙的时候需要的资源给提前规划好了，就算是不用也给你留出来了，通过牺牲空间来追求的是极致的确定性和效率。这种做法呢只在特定领域下是值得的，常见应用领域不会这么极端。比如说goalng的内存管理机制也是池化技术，不过池子的大小是会随着使用情况伸缩的，尽量减少了直接使用系统分配，也能提高效率。golang里面的sync.Pool就是系统提供的临时池，因为回收机制比较暴力，这个多半用在内存上，通信开销相当大的连接池什么的就不合适了。

基本原理就是有一类东西如果只使用一次就还给操作系统的话比较亏，借还的成本远远高于使用成本。那就先拿一些固定的提前放在自己手上，如果还是长期不够用就再拿一些，要是富余的量比较多那就还一点，至少保持最少的固定量。比如说线程，这个创建和销毁的开销就不小，如果只拿来做完一件小事就销毁很不划算。golang由于语言自带完整的协程机制，使用goroutine和chanel可以简单的完成工作池。通常线程池的实现中会有任务池，不是所有的任务来了都会触发新建线程的。这种缓冲机制对于CPU的计算型任务还有点，线程数跟CPU核心数量一个级别可以减轻切换压力。而对于IO任务就没有那么友好，毕竟还是要排队等待的，以golang这种将IO任务转化为CPU任务的语言能力来讲，应该是有性能优势的。再比如说连接，这个一般是tcp连接到某个系统，除了协议的开销还有一堆检查的应用层开销，如果只用一次就关掉也很亏。golang的sql driver就自带了连接池，创建的时候设定idle的连接数和最大的连接数即可，极大的方便了使用者。

### 拆分

任何系统都有薄弱环节，或者说性能瓶颈，成功的找到瓶颈变成为调优的第一步。受限于团队的能力或者环境，有时候即使找到瓶颈也不一定有办法处理的好。比如能力加入不了方案，硬件投入成本太高，或者复杂度太高而维护代价高昂等等。不管做法的结果如何，核心思想都是看看能不能化整为零挨个消灭。

比如数据库视业务的属性是有不同的拆分方式的。从纯技术的角度来讲，如果能够容忍一定的读取延迟，主从做读写分离就很好用（大事务这种自己作死的行为导致大的延迟就不要怪数据库了）。极端情况下主库挂掉导致数据不一致也还有HA能够让丛库接管。虽说主从发往丛库的binlog是后台线程，但丛库弄的太多了导致复制数据量大也会影响到主库的性能。延迟嘛解决方案就是尽量不要那么快去访问丛库（主要是读取自己刚写入的数据读不到就会产生思维混乱），而其他人的写入你延迟看到很正常而且容易接受。看业务情况，最好是业务层面优化（比如说本地记录，消息队列等等能够在不去查数据库的情况下拿到完整的数据），不好弄的话考虑加个缓存，实在不行就去主库读。主从的读写分离大多需要付出的额外代价，就是添加数据库的代理（例如mycat），客户端不会直连数据库，有点性能损失。除非你把这些出直接内嵌到业务代码中，在特定环境下这种做法也是可以的。从半技术半业务的角度来看的话，还可以直接切分数据。这就是各种hash，业务标记之类的找出一个区分度，然后把数据弄到不同的库中，写数据就不是只写一个库了，自然TPS性能就上来了。按照业务来拆分就是垂直拆分，这有个意外的好处，就是部分数据库的故障不会导致全局不可用，相互正交的业务是不受影响的。有时候即使你把业务都拆好了，也避免不了某个业务的数据量爆表，这种单点核心业务往往是公司的明星业务。对应的方式是水平拆分，就是hash的做法，同一类的数据用多个库或者表。这样在以后的读写中都得先做一次定位，也就是前面的标记。

前面的拆分基本上都是为了处理除了传统的关系模型，但有可能业务实在不够关系模型以至于方向搞错了问题还解决不好。有些东西还真的没啥关系，那就只能是另外一种nosql的模型来处理了。倒排索引和自动分区这两个技术还是很有吸引力的，前提是你的运维水平要高点才行。nosql这个词语历史比较久了，意思也在微妙的变化中。整体来看，在商业环境中其所占用的市场份额并不是大头。真的业务复杂的场景中能力有限，适合一些对事务要求不高但对存储和检索比较依赖的后台业务。因为小众所以懂的人也少，如果玩不转的话业务数据量上来了也很有可能把你带坑里去。

### 热点复用

这个基本上是大部分系统都拥有的特征，看似随机其实还是有区分度的。如果说社会是由人来驱动的话，那基本上是在不停的追逐新的话题，进而导致产业的更新与替代。无论如何，每一个周期总会有自己的风口，所以好钢用在刀刃上就是这个道理。redis的缓存是拿相对于硬盘来说比较贵的内存来做的，好处就是常用的数据大概率能够直接在内存中找到，本质上还是空间换时间的游戏。CPU的几级缓存也是同样的道理，就是成本不一样，是用比较贵的空间来换时间。看得多了这种感觉越来越强，时间至少在现阶段人类水平是玩不出什么花样的，反而随着技术的进步空间成本是越来越低。空间换时间应该是今后的一个主线，hash之类的会有长久的发展。等我党搞定了存储芯片产业之后，说不好哪天内存的价格再降低一个级别都是有可能的，随便一个民用机器都是几十GB的内存可能成为一种新常态。


