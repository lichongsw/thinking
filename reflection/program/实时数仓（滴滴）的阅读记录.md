## 实时数仓在滴滴的实践和落地

导读：随着滴滴业务的高速发展，业务对于数据时效性的需求越来越高，而伴随着实时技术的不断发展和成熟，滴滴也对实时建设做了大量的尝试和实践。本文主要以顺风车这个业务为引子，从引擎侧、平台侧和业务侧各个不同方面，来阐述滴滴所做的工作，分享在建设过程中的经验。

<!--数据已经是比较新而且极容易形成垄断的竞争力，如果是头部公司那一定对这个市场的数据拿捏的非常好（至少在国内就是这样的，技术大家都可以学习开源的产品和思维，自己来改造适应业务。只有极少数技术是一般公司玩不太好的，实时数仓就是这种。这个做好了简直可以把业务做出灵魂来）可惜没有机会经历这样的建设，这个技术对业务的贡献是非常大而且可以让技术人明白业务的重点（简单来说你知道大佬们关心什么业务指标，以及指标的变化在行业内的含义。不然一直用各种技术做业务的翻译是没啥意思的！就跟商务谈判的翻译一样，会各种语言的人如果不明白谈判上的你来我往，不懂得气势和立场上的轻微改变是没有办法帮助雇主表达出最准确的意图的。）-->



# **1. 实时数仓建设目的**

![img](https://mmbiz.qpic.cn/mmbiz_png/jE5bOw22iaBtFvdK9icOj3ibAXa8W3tqZ2lQDEaA5XcCDJ5cVIic2221PzXcw0oo69kvia8ojgPZnEV4jPxZURBln4A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

随着互联网的发展进入下半场，数据的时效性对企业的精细化运营越来越重要，商场如战场，在每天产生的海量数据中，如何能实时有效的挖掘出有价值的信息， 对企业的决策运营策略调整有很大帮助。
其次从智能商业的角度来讲，数据的结果代表了用户的反馈，获取结果的及时性就显得尤为重要，快速的获取数据反馈能够帮助公司更快的做出决策，更好的进行产品迭代，实时数仓在这一过程中起到了不可替代的作用。

<!--换个非技术的立场，业务真的是否需要实时数仓是不一定的。当然如果不用花太多钱，这个还是可以有的。你想想哪天作为一个分管方向的副总，在出篓子的第一时间被问到当前的状态的时候是不是很慌。这个可不是技术总监的特权，各种财务，安全，公共关系，供应链上的大佬都需要掌控态势。例如年度趋势，月度的目标偏离，近一周的事件流水，以及最新的锅的状态（还没有捅出篓子的）-->



## 1.1 解决传统数仓的问题



从目前数仓建设的现状来看，实时数仓是一个容易让人产生混淆的概念，根据传统经验分析，数仓有一个重要的功能，即能够记录历史。通常，数仓都是希望从业务上线的第一天开始有数据，然后一直记录到现在。但实时流处理技术，又是强调当前处理状态的一个技术，结合当前一线大厂的建设经验和滴滴在该领域的建设现状，我们尝试把公司内实时数仓建设的目的定位为，以数仓建设理论和实时技术，**解决由于当前离线数仓数据时效性低解决不了的问题**。<!--这个说法让人感觉是技术驱动，或者说这里的东西算是公司业务的商业机密不能讲。下面这些原因是没有啥说服力的。后面两条讲的是资源，不是根本性的问题。第一条只是迫切需要数据辅助完成决策，但没有场景。回到以前看google时序数据库那篇文章的感受，技术文章一定要有代入感。-->



现阶段我们要建设实时数仓的主要原因是：



- 公司业务对于数据的实时性越来越迫切，需要有实时数据来辅助完成决策
- 实时数据建设没有规范，数据可用性较差，无法形成数仓体系，资源大量浪费
- 数据平台工具对整体实时开发的支持也日渐趋于成熟，开发成本降低

<!--还是需要猜测一下滴滴业务上迫切需要的意图。从业务上讲滴滴对实时的需求应该远远比不上今日头条这类产品。可能这个环节有点啰嗦，但如果不去思考现实中究竟有啥问题驱动了这个技术的落地，一轮一轮的去看各种新的技术到此一游，到头来还真的是过目即忘。-->

<!--可以从两个大方面来思考：一是业务利益不能最大化（举例十个锅五个盖子的时候换锅盖的效率不高就赚不到最多的钱），二是对用户的伤害深（举例大小不合适的盖子）。-->

<!--首先从利益出发（必然的首选）：-->

1. <!--首先滴滴在国内基本上没有成型的竞争对手，可能在不同的城市会有一些地方性的小玩家。全国而言其他头部互联网公司的产品如果不持续撒钱，根本不会形成太大的冲击。在业务量比较稳的情况下，这个真不用实时-->
2. <!--假设第1条成立，那么就是怎么吃下蛋糕的问题了（这个可不简单）。例如司机在线的数量大于叫车订单的数量才能吃得下全部蛋糕。但司机数量过多就会形成恶性竞争，虽然这一具体时段的蛋糕全部吃下了但伤害了用来吃蛋糕的有生力量，可能因小失大。有这么几个指标应该是需要按照区域实时监控的（出行是一个线下行为，地理位置是直接影响所有业务的底层原变量）：**已上线司机使用率**（正在服务订单中的司机在全部上线司机中的占比，占比过高就得提高订单的价格或者发补贴增加司机上线的动力，原则就是吃掉最多的订单给平台赚最多的钱），**正在操作打车界面的乘客与已上线但未使用司机的比值**（这些是30秒之后可能变成业务订单的司机与乘客组合，安全的比值可以让乘客得到可接受的反馈，顺利生成订单）-->

<!--之后从伤害出发：-->

<!--像价格高于自己的心里价位，服务低于自己的预期这些是不需要实时，都可以事后统计。实时的伤害可能反应的是博弈关系，究竟是坑乘客还是司机多一点应该是有策略的。-->

1. <!--对用户的伤害。用户打不到车那就是严重的运营问题了，不仅损失了订单还给用户很差的体验。这种情况只能坑司机，例如超短途订单，拥堵订单，夜班订单之类可能降低司机的效率的活需要被监控，不厚道的说看看这类订单的承接力度，如果可以满足用户需求就不需要平台去补助了。世界不是平的，有压力就有动力。但对于那种大把加价收取用户费用后少量或者不返还司机的策略个人表示鄙视。-->
2. <!--对司机的伤害。司机接不到单也是运营问题。虽然订单没有损失，但司机数量太多会降低司机的效率和积极性。-->

## 1.2 **实时数仓的应用场景**



- 实时OLAP分析：OLAP分析本身就是数仓领域重点解决的问题，基于公司大数据架构团队提供的基于Flink计算引擎的stream sql工具，kafka和ddmq(滴滴自研)等消息中间件，druid和ClickHouse等OLAP数据库，提升数仓的时效性能力，使其具有较优的实时数据分析能力。
- 实时数据看板：这类场景是目前公司实时侧主要需求场景，例如“全民拼车日”订单和券花销实时大屏曲线展示，顺风车新开城当日分钟级订单侧核心指标数据展示，增长类项目资源投入和收益实时效果展示等。<!--从举例上看目标还是有点短，用来监控活临时投入的产出大材小用了。-->
- 实时业务监控：滴滴出行大量核心业务指标需要具备实时监控能力，比如安全指标监控，财务指标监控，投诉进线指标监控等。<!--这些其实都不是业务上的诉求，只是相关负责人的掌控力的体现。是否需要实时也是看情况的，例如乘客端的一键报警真的有人实时响应并联动公安吗？不见得有这种流程-->
- 实时数据接口服务：由于各业务线之间存在很多业务壁垒，导致数仓开发很难熟悉公司内全部业务线，需要与各业务线相关部门在数据加工和数据获取方面进行协作，数仓通过提供实时数据接口服务的方式，向业务方提供数据支持。<!--数据的视图本来就是跟着业务走的，数仓只需要提供好各种接口获取数据，至于数据跨越业务线的聚合这个数仓真的可以不用负责的。-->





![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbp8NIfRGT430D0f4xia5VnJY9CGbxjiazXicyGJNbRaxiaJKDvTsCffMLdQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)







# 2. 滴滴顺风车实时数仓建设举例

![img](https://mmbiz.qpic.cn/mmbiz_png/jE5bOw22iaBtFvdK9icOj3ibAXa8W3tqZ2lQDEaA5XcCDJ5cVIic2221PzXcw0oo69kvia8ojgPZnEV4jPxZURBln4A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在公司内部，我们数据团队有幸与顺风车业务线深入合作，在满足业务方实时数据需求的同时，不断完善实时数仓内容，通过多次迭代，基本满足了顺风车业务方在实时侧的各类业务需求，初步建立起顺风车实时数仓，完成了整体数据分层，包含明细数据和汇总数据，统一了DWD层，降低了大数据资源消耗，提高了数据复用性，可对外输出丰富的数据服务。



**数仓具体架构如下图所示：**



![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbCxoE2aMoFbLPRGbGibjwbC8tbIMPPWrXNjwY9ozzCwNaeVB66qBTVwg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



从数据架构图来看，顺风车实时数仓和对应的离线数仓有很多类似的地方。例如分层结构；比如ODS层，明细层，汇总层，乃至应用层，他们命名的模式可能都是一样的。但仔细比较不难发现，两者有很多区别：
与离线数仓相比，实时数仓的层次更少一些。从目前建设离线数仓的经验来看，数仓的数据明细层内容会非常丰富，处理明细数据外一般还会包含轻度汇总层的概念，另外离线数仓中应用层数据在数仓内部，但实时数仓中，app应用层数据已经落入应用系统的存储介质中，可以把该层与数仓的表分离。应用层少建设的好处：实时处理数据的时候，每建一个层次，数据必然会产生一定的延迟。汇总层少建的好处：在汇总统计的时候，往往为了容忍一部分数据的延迟，可能会人为的制造一些延迟来保证数据的准确。举例，在统计跨天相关的订单事件中的数据时，可能会等到 00:00:05 或者 00:00:10再统计，确保 00:00 前的数据已经全部接受到位了，再进行统计。所以，汇总层的层次太多的话，就会更大的加重人为造成的数据延迟。

与离线数仓相比，实时数仓的数据源存储不同在建设离线数仓的时候，目前滴滴内部整个离线数仓都是建立在 Hive 表之上。但是，在建设实时数仓的时候，同一份表，会使用不同的方式进行存储。比如常见的情况下，明细数据或者汇总数据都会存在 Kafka 里面，但是像城市、渠道等维度信息需要借助Hbase，mysql或者其他KV存储等数据库来进行存储。

**接下来，根据顺风车实时数仓架构图，对每一层建设做具体展开：**

<!--（鉴于首次接触数仓相关技术，贴图解释一下特殊名词。数仓这个以后有机会还是要深入的学习一下，看看各大公司是怎么利用这个优质资产的）-->

<!--ODS: 机器与程序员玩的地方-->

<!--DWD：领域或者说行业维度来整理数据，是产品们玩的地方。行业资深分析师或者产品经理居多-->

<!--DWS：各部门总监们玩的地方，例如各个行业都有财务总监，技术总监。大佬喜欢看自己管理的汇总-->

<!--ADS：数据的终端用户（可能是乘客和司机这类直接用户，也有可能是滴滴监控/客服中心的员工这类间接用户）-->

<!--DIM: 按照维度来分数据，应该是高层玩的地方。吹水的时候应该经常用这里的数据，多少城市，多少用户，多少成交之类的。这个视图可以说是战役级别的玩家关心的，例如抽调一个牛逼的城市负责人去刚起步的城市打攻坚战，再例如比较豪的乘客就会被杀熟没啥优惠券。能够从一个维度去制定行动的绝逼是强力的指挥部门，这些操作影响面通常是很大甚至是全局的-->

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020082620230429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JlaWlzQmVp,size_1,color_FFFFFF,t_70#pic_center)

## 2.1ODS 贴源层建设

根据顺风车具体场景，目前顺风车数据源主要包括订单相关的binlog日志，冒泡和安全相关的public日志，流量相关的埋点日志等。这些数据部分已采集写入kafka或ddmq等数据通道中，部分数据需要借助内部自研同步工具完成采集，最终基于顺风车数仓ods层建设规范分主题统一写入kafka存储介质中。
**命名规范：**ODS层实时数据源主要包括两种。
一种是在离线采集时已经自动生产的DDMQ或者是Kafka topic，这类型的数据命名方式为采集系统自动生成规范为：cn-binlog-数据库名-数据库名 eg：cn-binlog-ihap_fangyuan-ihap_fangyuan一种是需要自己进行采集同步到kafka topic中，生产的topic命名规范同离线类似：ODS层采用：realtime_ods_binlog_{源系统库/表名}/ods_log_{日志名} eg: realtime_ods_binlog_ihap_fangyuan

## 2.2 DWD 明细层建设

**根据顺风车业务过程作为建模驱动，基于每个具体的业务过程特点，构建最细粒度的明细层事实表；结合顺风车分析师在离线侧的数据使用特点，将明细事实表的某些重要维度属性字段做适当冗余，完成宽表化处理，之后基于当前顺风车业务方对实时数据的需求重点，重点建设交易、财务、体验、安全、流量等几大模块；该层的数据来源于ODS层，通过大数据架构提供的Stream SQL完成ETL工作，对于binlog日志的处理主要进行简单的数据清洗、处理数据漂移和数据乱序，以及可能对多个ODS表进行Stream Join，对于流量日志主要是做通用的ETL处理和针对顺风车场景的数据过滤，完成非结构化数据的结构化处理和数据的分流；该层的数据除了存储在消息队列Kafka中，通常也会把数据实时写入Druid数据库中，供查询明细数据和作为简单汇总数据的加工数据源。
**命名规范：**DWD层的表命名使用英文小写字母，单词之间用下划线分开，总长度不能超过40个字符，并且应遵循下述规则：realtime_dwd_{业务/pub}_{数据域缩写}_[{业务过程缩写}]_[{自定义表命名标签缩写}]
{业务/pub}：参考业务命名{数据域缩写}：参考数据域划分部分{自定义表命名标签缩写}：实体名称可以根据数据仓库转换整合后做一定的业务抽象的名称，该名称应该准确表述实体所代表的业务含义
**样例：**realtime_dwd_trip_trd_order_base 

## 2.3 DIM 层

公共维度层，基于维度建模理念思想，建立整个业务过程的一致性维度，降低数据计算口径和算法不统一风险；DIM 层数据来源于两部分：一部分是Flink程序实时处理ODS层数据得到，另外一部分是通过离线任务出仓得到；DIM 层维度数据主要使用 MySQL、Hbase、fusion(滴滴自研KV存储) 三种存储引擎，对于维表数据比较少的情况可以使用 MySQL，对于单条数据大小比较小，查询 QPS 比较高的情况，可以使用 fusion 存储，降低机器内存资源占用，对于数据量比较大，对维表数据变化不是特别敏感的场景，可以使用HBase 存储。
**命名规范：**DIM层的表命名使用英文小写字母，单词之间用下划线分开，总长度不能超过30个字符，并且应遵循下述规则：dim_{业务/pub}_{维度定义}[_{自定义命名标签}]：
{业务/pub}：参考业务命名{维度定义}：参考维度命名{自定义表命名标签缩写}：实体名称可以根据数据仓库转换整合后做一定的业务抽象的名称，该名称应该准确表述实体所代表的业务含义
**样例：**dim_trip_dri_base



## 2.4 DWM 汇总层建设

在建设顺风车实时数仓的汇总层的时候，跟顺风车离线数仓有很多一样的地方，但其具体技术实现会存在很大不同。
第一：对于一些共性指标的加工，比如pv，uv，订单业务过程指标等，我们会在汇总层进行统一的运算，确保关于指标的口径是统一在一个固定的模型中完成。对于一些个性指标，从指标复用性的角度出发，确定唯一的时间字段，同时该字段尽可能与其他指标在时间维度上完成拉齐，例如行中异常订单数需要与交易域指标在事件时间上做到拉齐。
第二：在顺风车汇总层建设中，需要进行多维的主题汇总，因为实时数仓本身是面向主题的，可能每个主题会关心的维度都不一样，所以需要在不同的主题下，按照这个主题关心的维度对数据进行汇总，最后来算业务方需要的汇总指标。在具体操作中，对于pv类指标使用Stream SQL实现1分钟汇总指标作为最小汇总单位指标，在此基础上进行时间维度上的指标累加；对于uv类指标直接使用druid数据库作为指标汇总容器，根据业务方对汇总指标的及时性和准确性的要求，实现相应的精确去重和非精确去重。
第三：汇总层建设过程中，还会涉及到衍生维度的加工。在顺风车券相关的汇总指标加工中我们使用Hbase的版本机制来构建一个衍生维度的拉链表，通过事件流和Hbase维表关联的方式得到实时数据当时的准确维度
**命名规范：**DWM层的表命名使用英文小写字母，单词之间用下划线分开，总长度不能超过40个字符，并且应遵循下述规则：realtime_dwm_{业务/pub}_{数据域缩写}_{数据主粒度缩写}_[{自定义表命名标签缩写}]_{统计时间周期范围缩写}：
{业务/pub}：参考业务命名{数据域缩写}：参考数据域划分部分{数据主粒度缩写}：指数据主要粒度或数据域的缩写，也是联合主键中的主要维度{自定义表命名标签缩写}：实体名称可以根据数据仓库转换整合后做一定的业务抽象的名称，该名称应该准确表述实体所代表的业务含义{统计时间周期范围缩写}：1d:天增量；td:天累计(全量)；1h:小时增量；th:小时累计(全量)；1min:分钟增量；tmin:分钟累计(全量)
**样例：**realtime_dwm_trip_trd_pas_bus_accum_1min



## 2.5 APP 应用层

**该层主要的工作是把实时汇总数据写入应用系统的数据库中，包括用于大屏显示和实时OLAP的Druid数据库(该数据库除了写入应用数据，也可以写入明细数据完成汇总指标的计算)中，用于实时数据接口服务的Hbase数据库，用于实时数据产品的mysql或者redis数据库中。
**命名规范：**基于实时数仓的特殊性不做硬性要求





# 3.顺风车实时数仓建设成果

截止目前，一共为顺风车业务线建立了增长、交易、体验、安全、财务五大模块，涉及40+的实时看板，涵盖顺风车全部核心业务过程，**实时和离线数据误差<0.5%**，是顺风车业务线数据分析方面的有利补充，为顺风车当天发券动态策略调整，司乘安全相关监控，实时订单趋势分析等提供了实时数据支持，提高了决策的时效性。同时建立在数仓模型之上的实时指标能根据用户需求及时完成口径变更和实时离线数据一致性校验，大大提高了实时指标的开发效率和实时数据的准确性，也为公司内部大范围建设实时数仓提供了有力的理论和实践支持。

# 4. 实时数仓建设对数据平台的强依赖![img](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)

目前公司内部的实时数仓建设，需要依托数据平台的能力才能真正完成落地，包括StreamSQL能力，数据梦工程StreamSQL IDE环境和任务运维组件，实时数据源元数据化功能等。
![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbMzqBBia3KFQHXyajibhIMlzWkAjB602mWOhnEG0nljvjOY5tarJOibTibQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 4.1 基于StreamSQL的实时数据需求开发

StreamSQL是滴滴大数据引擎部在Flink SQL 基础上完善后形成的一个产品。
使用 StreamSQL 具有多个优势：
**描述性语言：**业务方不需要关心底层实现，只需要将业务逻辑描述出来即可。<!--（看来大公司是给机会做DSL的，而且是很有性价比的）-->

**接口稳定：**Flink 版本迭代过程中只要 SQL 语法不发生变化就非常稳定。

**问题易排查：**逻辑性较强，用户能看懂语法即可调查出错位置。

**批流一体化：**批处理主要是 HiveSQL 和 Spark SQL，如果 Flink 任务也使用 SQL 的话，批处理任务和流处理任务在语法等方面可以进行共享，最终实现一体化的效果。<!--这点值得赞扬，从技术的角度上统一了业务，给架构师点赞-->

**StreamSQL 相对于 Flink SQL （1.9之前版本）的完善：****
**完善 DDL：**包括上游的消息队列、下游的消息队列和各种存储如 Druid、HBase 都进行了打通，用户方只需要构建一个 source 就可以将上游或者下游描述出来。**

**内置消息格式解析：**消费数据后需要将数据进行提取，但数据格式往往非常复杂，如数据库日志 binlog，每个用户单独实现，难度较大。StreamSQL 将提取库名、表名、提取列等函数内置，用户只需创建 binlog 类型 source，并内置了去重能力。对于 business log 业务日志 StreamSQL 内置了提取日志头，提取业务字段并组装成 Map 的功能。对于 json 数据，用户无需自定义 UDF，只需通过 jsonPath 指定所需字段。**扩展UDX：**丰富内置 UDX，如对 JSON、MAP 进行了扩展，这些在滴滴业务使用场景中较多。支持自定义 UDX，用户自定义 UDF 并使用 jar 包即可。兼容 Hive UDX，例如用户原来是一个 Hive SQL 任务，则转换成实时任务不需要较多改动，有助于批流一体化。
**Join 能力扩展：**
① 基于 TTL 的双流 join：在滴滴的流计算业务中有的 join 操作数据对应的跨度比较长，例如顺风车业务发单到接单的时间跨度可能达到一个星期左右，如果这些数据的 join 基于内存操作并不可行，通常将 join 数据放在状态中，窗口通过 TTL 实现，过期自动清理。② 维表 join 能力：维表支持 HBase、KVStore、Mysql 等，同时支持 inner、left、right、full join 等多种方式。 

## 4.2 基于数据梦工厂的StreamSQL IDE和任务运维

StreamSQL IDE：
**提供常用的SQL模板：**在开发流式 SQL 时不需要从零开始，只需要选择一个 SQL 模板，并在这个模板之上进行修修改改即可达到期望的结果

**提供 UDF 的库：**相当于一个库如果不知道具有什么含义以及如何使用，用户只需要在 IDE 上搜索到这个库，就能够找到使用说明以及使用案例，提供语法检测与智能提示

**提供代码在线DEBUG能力：**可以上传本地测试数据或者采样少量 Kafka 等 source 数据 debug，此功能对流计算任务非常重要。提供版本管理功能，可以在业务版本不断升级过程中，提供任务回退功能。

**任务运维：**任务运维主要分为四个方面

**日志检索：**Flink UI 上查询日志体验非常糟糕，滴滴将 Flink 任务日志进行了采集，存储在 ES 中，通过 WEB 化的界面进行检索，方便调查。

**指标监控：**Flink 指标较多，通过 Flink UI 查看体验糟糕，因此滴滴构建了一个外部的报表平台，可以对指标进行监控。

**报警：**报警需要做一个平衡，如重启报警有多类如 ( 机器宕机报警、代码错误报警 )，通过设置一天内单个任务报警次数阈值进行平衡，同时也包括存活报警 ( 如 kill、start )、延迟报警、重启报警和 Checkpoint 频繁失败报警 ( 如 checkpoint 周期配置不合理 ) 等。

**血缘追踪：**实时计算任务链路较长，从采集到消息通道，流计算，再到下游的存储经常包括4-5个环节，如果无法实现追踪，容易产生灾难性的问题。例如发现某流式任务流量暴涨后，需要先查看其消费的 topic 是否增加，topic 上游采集是否增加，采集的数据库 DB 是否产生不恰当地批量操作或者某个业务在不断增加日志。这类问题需要从下游到上游、从上游到下游多方向的血缘追踪，方便调查原因。

## 4.3 基于数据梦工厂的实时数据源元数据化(meta化表)

将topic引入成实时表，metastore统一管理元数据，实时开发中统一管理DDL过程。对实时数仓来说，通过元数据化，可以沉淀实时数仓的建设成果，使数仓建模能更好的落地 ![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbXrdZLyghJZgN1tRpicR9SdtvTu6dLaKISRAo4S5PyV4icV6I9Ns9tnhw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
目前数据梦工厂支持的元数据化实时数据源包括Postgre、DDMQ、Mysql、Druid、ClickHouse、Kylin、Kafka。

# 5.面临的挑战和解决方案思考

虽然目前滴滴在实时数仓建设方面已初具规模，但其面临的问题也不容忽视。

## 5.1 实时数仓研发规范

**问题：**为了快速响应业务需求，同时满足数仓的需求开发流程，迫切需要建设一套面向实时数据开发的规范白皮书，该白皮书需要涉及需求对接、口径梳理、数据开发、任务发布、任务监控、任务保障
**目前解决方案：**目前由数据BP牵头，制定了一套面向实时数据指标的开发规范：<!--看起来流程性的东西都需要完善-->
![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbcCPrusBpu29eAlCJKko4s3lfsjQgOst24Z2iaeeyw5dqJkxHiaakjticg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
**常规流程：**需求方提出需求，分析师对接需求，提供计算口径，编写需求文档。之后由数仓BP和离线数仓同学check计算口径，并向实时数仓团队提供离线hive表，实时数仓同学基于离线hive表完成数据探查，基于实时数仓模型完成实时数据需求开发，通过离线口径完成数据自查，最终交付给分析师完成二次校验后指标上线。
**口径变更--业务方发起：**业务方发起口径变更，判断是否涉及到实时指标，数仓BP对离线和实时口径进行拉齐，向离线数仓团队和实时数仓团队提供更口口径和数据源表，实时数仓团队先上测试看板，验收通过后切换到正式看板
**存在的不足：**
当针对某个业务进行新的实时数据建设时，会有一个比较艰难的初始化过程，这个初始化过程中，会和离线有较多耦和，需要确定指标口径，数据源，并进行大量开发测试工作 在指标口径发生变更的时候，需要有一个较好的通知机制，目前还是从人的角度来进行判断。<!--游戏项目中也是这样，特别是技术以及美术团队来自各自的技术中心的时候，一件事经常一波一波的换人，磨合和变更都比较麻烦。-->

## 5.2 离线和实时数据一致性保证

**目前解决办法：**由业务、BP、离线数仓共同保证数据源、计算口径与离线一致，数据加工过程，逐层与离线进行数据比对，并对指标结果进行详细测试，数据校验通过并上线后，根据离线周期进行实时和离线数据的校验
![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbufRGyEWaFV5tqJqXQMoev0pMYnRxHxumCiadkpNAu3dEpIQLSBibpamA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
**待解决的问题：**结合指标管理工具，保证指标口径上的一致性，扩展数据梦工厂功能，在指标加工过程中，增加实时离线比对功能，降低数据比对成本。

# 6.未来展望—批流一体化

虽然 Flink 具备批流一体化能力，但滴滴目前并没有完全批流一体化，希望先从产品层面实现批流一体化。通过 Meta 化建设，实现整个滴滴只有一个 MetaStore，无论是 Hive、Kafka topic、还是下游的 HBase、ES 都定义到 MetaStore 中，所有的计算引擎包括 Hive、Spark、Presto、Flink 都查询同一个 MetaStore，实现整个 SQL 开发完全一致的效果。根据 SQL 消费的 Source 是表还是流，来区分批处理任务和流处理任务，从产品层面上实现批流一体化效果。<!--流处理与批处理的差异是挺大的，在DDIA这本书中讲的挺多，以及处理中失效模式的应对。一体化的处理对于技术来说是一件好事，但从业务的角度考虑要讲究效益。-->
![img](https://mmbiz.qpic.cn/mmbiz_png/1wBZCGiaYqBESVIPKfAvsfztD0z88sbVbnnelSZwOyaOofP5w0Lr17NHw5GwiaOb3ydMru069df2z5DaaazicAW2g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
![img](https://mmbiz.qpic.cn/mmbiz_png/jE5bOw22iaBuWAdlAqTgfBBO17X2xFt6H3aF9JyDT8ibeCQdyxMjZq6UL0LLd82Fu9HmdgSLsTU6JPb43wjn6HAQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
**团队介绍**
本文内容涉及三个滴滴云平台事业群团队，云平台事业部大数据架构团队，主要负责大数据底层引擎的建设，建设并维护公司内部，离线、OLAP、实时、保障等底层引擎。云平台事业部大数据平台部，主要负责公司内部通用平台建设，包括一站式开发平台，内置业界沉淀多年的数据开发流程及规范，满足用户对数据开发、数据安全、质量管理、数据管理需求。云平台事业部实时数仓团队，主要负责滴滴内部各业务线的实时数据建设、以及实时数据规范的沉淀，中间层的数据建设等。

