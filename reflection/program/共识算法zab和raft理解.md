# 按自己的理解记忆

前有paxos作为理论先锋，后又业界从使用角度实现的zab（zookeeper）和raft（etcd），还是要理解一波这两种主流的共识算法机制。这里仅仅从共识和一致性的角度出发，这两兄弟实现的东西是在太多了，典型的瑞士军刀，能做配置与订阅，能做负载均衡，能做服务发现，甚至于还能够搞小范围的分布式kv。本质上来讲他两都解决分布式系统的协调和元数据的存储，衍生出了各种功能。

## zab

ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。这个名字不太全面，容错时的崩溃恢复没有反应出来。

1. 一主多备的集群模式，跟mysql差不多的用法。这种套路的写入上限很容易受到单主的性能影响。mysql的半同步复制是至少有一个从节点确认了更新请求后主节点才返回请求结果给客户端。ZAB要求更加严格，需要一半以上的从节点确认后才能提交。过往中mysql这个地方也做的不好，从5.6版本的AFTER_COMMIT到5.7版本的AFTER_SYNC的变化可以看出宁可从库多数据也不能让主库失败丢数据，这是后面的主流做法。
2. 每一个客户端的请求会分配一个全局事务ID，zxid就是用来保证事务（消息）的顺序，之后消息丢到队列中异步发给从节点。消息在主从节点上以相同的顺序执行，得出相同的结果。
3. 这个模式有明显的单点，明确的问题再配合针对性的解决方案也是经得住考验的，怕的是问题遮遮掩掩没有信心。崩溃恢复中选举的原理看起来清晰，但实际上不简单。没有领导的时候大家一开始都推荐自己，但是发现比自己推荐的人更好的候选者的时候就会改口推荐更好的候选者，是一个网状的广播通信。一轮投票中可以该自己的票（相当于投票多次），做法有点见风使舵的意思，结果是一直收敛，直到大部分人意见一致选举结束，只用一轮就可以完成选举（时间不定）。即使拿到了大部分的票也不见得能够当选，而是要等待200ms看看有没有比你更加牛逼的人。所以zab强相遇通常拥有最大zxid的节点（数据最全的）成为新的主节点，简单地说下任就是还活着的人里面最牛逼的那个，有点像是战场上的前线指挥官机制。如果前面失效的主节点没有来得及提交或者提交没有通知到任何一个从节点就挂了，结果不会有任何从节点拥有这一次的提交消息，也就是这个事务最终被丢弃失败了。如果前面失效的主节点已经提交了但还没有广播到全部从节点就挂了，这样一定有某些从节点拥有提交的消息，后面被选中成为新的主节点后事务最终被确认就成功了。
4. 恢复的时候新选中的主节点需要出来清理现场，就是新官上任三把火，前任的烂摊子还是收拾干净。例如查一下最大的zxid在从节点中的确认情况，先改朝换代然后把一样的消息换个新朝代的马甲后还是按规矩询问全部从节点，等得到半数以上的确认后再告诉客户端执行成功，即崩溃恢复完成。即使前朝卷土从来也没有机会了，一朝天子一朝臣，从节点只听命于新的老大了。
5. 整个集群在两个模式之间转换，常态下就是消息广播，就是一个简化版本的2PC，但通过队列解决了同步阻塞问题。真的元首跪了（被跪了）就进行崩溃恢复，通过崩溃恢复引入新的领导者解决了单点问题。

## raft

1. 也是典型的一主多从，这里的角色区分有三种，leader，follower，candidate，就是平头百姓，候选人，当权者。角色变换的规则更加简单，就是一种随机的时间（150~300毫秒）。刚开始所有的人都是百姓，如果没有当权者过来发福利百姓就想自己上位发福利（可以理解成实现中的消息或者心跳），只是每个百姓能够容忍没有福利的时间不一样。最先忍不住的百姓先跳出来毛遂自荐承诺自己当选后给百姓发福利（百姓觉得你比他牛逼才会给你投票，不然是不会投票给你的，这点可不是广播而是双向通信，对拉票的人来说是有结果的）。如果能够得到多数百姓的支持那就立刻当选了，这里的选举机制不同于zab，英雄也是要看时势的。如果一个没有你牛逼的人刚好早一点开始竞选，他又在更早时间内就获得了足够多的支持的话那你就没有机会了。所以机会还是要自己主动去争取的，实力强大不等于一定成功，这点raft倾向于选出一个足够好的人就可以了。上任后要尽量按照竞选承诺发福利（就是leader给follewer发送心跳），来确认自己的地位。这有点类似美国总统竞选，只要上任总统的任期即将结束，任何美国公民都可以竞选下一任总统，当选的前提就是得到优势票数。一旦上任后不守承诺（给百姓发福利），那不好意思各级官员要弹劾你，总统任期搞不好被提前结束，最先忍不住的百姓又要开始下一轮竞选了。
2. 技术上讲竞选是个公平的活，两个或者多个百姓同时忍不住了那就开始pk了。第一轮平分秋色的情况下，那就加赛再开启新一轮新的竞选，新一轮的竞选者出现同时忍不住的概率就小很多了。讲道理raft选举要快一点，毕竟一轮每个百姓只投票一次，收敛速度更快。只有极端情况下出现多轮才会加长选举时间。
3. 万一真的出事了，有人因为外因开始闹分裂了不一起玩了，那就变成了两个独立的分区了，新分裂出的区要开始自己的竞选。两个分区开始分别响应外部请求，如果有大部分百姓同意的话能够搞定外部事情。等到外因消失分裂结束了，旧区的头和百姓把以前外面请求的东西都给扔掉，然后接受最新的政策归顺了新区。为啥一定是新区完成大一统呢，估计还是为了实现简单，这个时候居然不是靠百姓的数量来比拼。是不是有风水轮流转的感觉，也许是为了一种平衡，让各个节点都有机会当领导。

## 其他区别

除开概念上的一致性，通信模型上的差异导致功能上两者还是有很多区别的：

1. zab的任意节点都可以服务于客户端，数据可能过时，但一定是多数节点确认过的。raft先随机节点找到leader后就只跟leader玩了，leader压力山大。对时效性有要求的话zab可以主动使用sync来拿到最新的数据，这样可以平衡性能很时效性，对业务更加友好。后期zookepper甚至出现了observer节点，这种不投票但能干只读的活。哪有这么听话的百姓，没有任何权利还可以干点活（那不是一点，现实中大部分业务都是写少读多的）。
2. raft的事务是双向rpc，leader是要等follower干完活的结果的。这点性能肯定不如zab的广播，leader发广播后不管了，等follower异步完成后再广播通知leader结果。广播在及时性上还有点优势，就是leader确认结果后通知小弟提交的代价不一样。raft舍不得rpc的成本，就等到下一个心跳顺便通知提交，而zab确认后就直接广播给小弟提交了。
3. 对于已有集群加入新的节点时，raft相对简单，leader会发心跳过来。zookeeper还是推荐自己，等到收到现有节点的回复后，判断已经有leader了就放弃自己的投票。raft在新节点的加入过程中是不会阻塞对外请求的，原理是leader维护了所有follower的连续索引。目前zookeeper的zxid并不要求连续，确认新来的follower与leader的差异的时候需要锁定leader当前的数据。会阻塞对外请求。
4. raft只有follower会检查心跳（单向），一段时间收不到就一定会发起新一轮的投票。zookeeper的leader检查心跳的回复有没有过半数，follower检查是否有心跳（双向）。
5. 处理前任的数据方式有差异。zab在投票过程中通过广播不停的交换知识取得一致，而raft是先选出来再同步数据。所以zab对前任的数据是激进策略，只要现任知道就可以认为多数人已经知道，可以提交。raft只能先全都不认，由现任来提议再询问一次结果后才能够提交。对于前任的假死，两者差不多，靠epoch和term这种递增的计数器来分别。

## zookeeper吐槽和etcd的崛起

对网络的明感程度和底层存储模型的差异导致使用起来的体验差别也不小：

1. zab是基于多杈树的内存节点，是一个文件储存模型。实现的功能大部分通过临时节点和回调，感觉起来就像你在操作底层的API来实现一个高层级的应用功能。而且要注意各个节点的数据并不一致，客户端获取最新的数据必须自己主动同步。
2. zab的watch机制的回调是一次性的。如果更新频率高，需要重复注册而且不能确保全部更新都不漏。
3. zab这种瑞士军刀的体验感给开发者不小负担，用起来要小心翼翼。Curator这个有Netflix牛逼公司实现的开源zookeeper框架拯救了不少公司和程序员。话说开源的红利就是这个讨论，总有牛逼公司把一个理论弄出工业级的产品给业界使用。中国的互联网公司欠开源世界一大笔，只有少数几家在给世界级的开源项目做贡献。
4. zab一旦出现网络隔离导致选举（leader在少数区），选举过程中大量的数据交换导致选举比较慢，这段时间没有leader就无法对外提供服务。也许短时间的1秒跨机房网络隔离导致的不可用就会被放大到整个选举周期。就算leader在多数区，那少数区没有leader也是无法提供服务的，就算服务只需要使用少数区机房内部的资源也不行。因为zab自身的问题破坏了应用的可用性，这事说起来可严重了。
5. 服务缺乏扩展性，不用加机器来取得线性的服务能力。加机器只是可靠性增强，这做法很不互联网。这在互联网公司来看是不可接受的，万一公司多元化出现服务规模的持续暴涨，那垂直划分成多个小集群只能缓解，哪一天要水平联通的时候你都没地方哭。
6. 储存缺乏扩展性，节点最多能管1M的数据，这完全不能用来做分布式存储。小范围的kv集合只能小打小闹。
7. 服务的健康直接绑定在tcp长连接的活性上，这限制了应用对于服务活性的理解。tcp可用不代表服务可用。

有问题的东西google是不能忍的，基于raft的etcd解决了一些问题，抢回了zookeeper的不少市场，确实可以算作年轻的继任者了。raft的实现简单不少，以至于国内有不少玩家基于开源开码自己整出一些新的东西。在分布式系统中的数据分为控制数据和应用数据。使用etcd的场景默认处理的数据都是控制数据。对于应用数据，勉强来说只推荐数据量很小，但是更新访问频繁的情况。

1. 同样单leader的机制决定了跨机房的问题是解决不掉的。
2. go比java的性能强一点，也一样有GC问题，但这方面不要质疑google的能力，随着go的迭代，性能绝对会做的比zk好。
3. 客户端的使用restful API基于http，用起来简单多了，轻松跨语言。
4. 支持ssl，安全性这块还是得到重视。
5. 可扩展性，每个实例每秒支持一千次级别的写操作。

## 不同场景下的选择

zookeeper这么流行不是没有道理的，它有自己非常擅长的领域。在粗粒度分布式锁，分布式选主，主备高可用切换等不需要高 TPS 支持的场景下有不可替代的作用，而这些需求往往多集中在大数据、离线任务等相关的业务领域，因为大数据领域，讲究分割数据集，并且大部分时间分任务多进程 / 线程并行处理这些数据集，但是总是有一些点上需要将这些任务和进程统一协调，这时候就是 ZooKeeper 发挥巨大作用的用武之地。它可用做一个分布式领域协调的王者，但是服务发现注册中心之类的真的不合适。
