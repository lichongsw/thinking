# 按自己的理解记忆

前有paxos作为理论先锋，后又业界从使用角度实现的zab（zookeeper）和raft（etcd），还是要理解一波这两种主流的共识算法机制。这里仅仅从共识和一致性的角度出发，这两兄弟实现的东西是在太多了，典型的瑞士军刀，能做配置与订阅，能做负载均衡，能做服务发现，甚至于还能够搞小范围的分布式kv。本质上来讲他两都解决分布式系统的协调和元数据的存储，衍生出了各种功能。

## zab

ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）。这个名字不太全面，容错时的崩溃恢复没有反应出来。

1. 一主多备的集群模式，跟mysql差不多的用法。这种套路的写入上限很容易受到单主的性能影响。mysql的半同步复制是至少有一个从节点确认了更新请求后主节点才返回请求结果给客户端。ZAB要求更加严格，需要一半以上的从节点确认后才能提交。过往中mysql这个地方也做的不好，从5.6版本的AFTER_COMMIT到5.7版本的AFTER_SYNC的变化可以看出宁可从库多数据也不能让主库失败丢数据，这是后面的主流做法。
2. 每一个客户端的请求会分配一个全局事务ID，zxid就是用来保证事务（消息）的顺序，之后消息丢到队列中异步发给从节点。消息在主从节点上以相同的顺序执行，得出相同的结果。
3. 这个模式有明显的单点，明确的问题再配合针对性的解决方案也是经得住考验的，怕的是问题遮遮掩掩没有信心。崩溃恢复中选举的原理看起来清晰，但实际上不简单。没有领导的时候大家一开始都推荐自己，但是发现还有比自己推荐的人更好的候选者的时候就会改口推荐更好的候选者。就是一个网状结构通过广播来不断刷新每个节点的公共认知（所有节点的投票情况）进而达到取得共识的过程。投票中不停的改自己的票（相当于投票多次），做法有点见风使舵的意思，结果确实是一直收敛，只用一轮就可以完成选举（时间不定）。即使拿到了大部分的票也不见得能够当选，而是要等待200ms看看有没有比你更加牛逼的人。所以zab倾向于拥有最大zxid的节点（数据最全的）成为新的主节点，简单地说下任就是还活着的人里面最牛逼的那个，有点像是战场上的前线指挥官机制。如果前面失效的主节点没有来得及提交或者提交没有通知到任何一个从节点就挂了，结果不会有任何从节点拥有这一次的提交消息，也就是这个事务最终被丢弃失败了。如果前面失效的主节点已经提交了但还没有广播到全部从节点就挂了，这样一定有某些从节点拥有提交的消息，后面被选中成为新的主节点后事务最终被确认就成功了。
4. 恢复的时候新选中的主节点需要出来清理现场，就是新官上任三把火，前任的烂摊子还是收拾干净。例如查一下最大的zxid在从节点中的确认情况，先改朝换代然后把一样的消息换个新朝代的马甲后还是按规矩询问全部从节点，等得到半数以上的确认后再告诉客户端执行成功，即崩溃恢复完成。即使前朝卷土从来也没有机会了，一朝天子一朝臣，从节点只听命于新的老大了。
5. 整个集群在两个模式之间转换，常态下就是消息广播，就是一个简化版本的2PC，但通过队列解决了同步阻塞问题。真的元首跪了（被跪了）就进行崩溃恢复，通过崩溃恢复引入新的领导者解决了单点问题。

## raft

1. 也是典型的一主多从，这里的角色区分有三种，leader，follower，candidate，就是平头百姓，候选人，当权者。角色变换的规则更加简单，就是一种随机的时间（150~300毫秒）。刚开始所有的人都是百姓，如果没有当权者过来发福利百姓就想自己上位发福利（可以理解成实现中的消息或者心跳），只是每个百姓能够容忍没有福利的时间不一样。最先忍不住的百姓先跳出来毛遂自荐竞选当期当权者（如果有人竞选以前任期，百姓会觉得你有点可爱，忽略你的存在。百姓觉得你比他牛逼才会给你投赞成票，否则给你一张拒绝票。这点可不是广播送达而是双向通信，对拉票的人来说是有结果的，投票的结果只告诉拉票的人。如果只有少数人赞成，那你就有点自知之明不先消停一会，等待牛人出现）。如果能够得到多数百姓的支持那就立刻当选了，这里的选举机制不同于zab，英雄也是要看时势的。如果一个没有你牛逼的人刚好早一点开始竞选，他又在更早时间内就获得了足够多的支持的话那你就没有机会了。所以机会还是要自己主动去争取的，实力强大不等于一定成功，这点raft倾向于选出一个足够好的人就可以了。上任后要尽量按照竞选承诺发福利（就是leader给follewer发送心跳），来确认自己的地位。这有点类似美国总统竞选，只要上任总统的任期即将结束，任何美国公民都可以竞选下一任总统，当选的前提就是得到优势票数。一旦上任后不守承诺（给百姓发福利），那不好意思各级官员要弹劾你，总统任期搞不好被提前结束，最先忍不住的百姓又要开始下一轮竞选了。
2. 技术上讲竞选是个公平的活，两个或者多个百姓同时忍不住了那就开始pk了。第一轮平分秋色的情况下，那就加赛再开启新一轮新的竞选，新一轮的竞选者出现同时忍不住的概率就小很多了。讲道理raft选举要快一点，毕竟一轮每个百姓只投票一次，收敛速度更快。只有极端情况下出现多轮才会加长选举时间。
3. 万一真的出事了，有人因为外因开始闹分裂了不一起玩了，那就变成了两个独立的分区了。万一头不在新分裂出的区（这个基本上表示改朝换代了，大部分人都不服从的话就玩不转了），新区就会选出新的头，就会出现有两个分区开始分别响应外部请求（分裂的只是少部分的话是选不出新的头的，少数人作乱相互乱咬，都得不到足够的支持）。这里有个预防措施，就是头会周期性的检测自己还有多少拥护者，如果没有多数人在拥护他就主动退位（这个才是真民主）。等到外因消失分裂结束了，少数区服从多数区完成大一统。大一统的过程中为了防止出现不必要的选举（还是因为太民主了，所以统一的过程中只要发福利超时就会出跳出来），这种做法会增加不必要的选举（虽然结果肯定是选不上的）。有个预选措施叫做预选，如果得不到大部分的支持就不能发起正式选举，这让少数派建立新朝代的机会都没有了。其实这个做法挺扯淡的，预选就是一种意向，百姓可以给所有预选者投票多次就是为了早点让有点能力的人发起真正的选举，刷掉一些明显不够好的人免得票都被分散了增加了拿到多数票的难度。
4. 为了进一步的降低正常任期内的异动，居然还限制拿了本届的福利后承诺不给新的竞选者投票。原理上有点像让那些跟着头的铁杆不给新区竞选投票，等头觉得不好意思了自己退位，然后再统一选举。越看细节越觉得raft并不公平，可能是为了降低实现和理解难度，够简单够好用一直都是这个行业的基本原则。
5. 整体来讲为了降低节点变更的冲击，原则上新任期的百姓基本不会理会就旧任期的消息，只有预选，心跳等除外。而且对于变更是保守策略，就是说用迭代的方式一次增加一个新成员，就是一个成员的变更相当于处理一条客户端的请求，产生一条log entry并需要得到大多数成员的认可。即使在集群启动时也是这样，通过变更的方式把配置中的所有成员一样加入。新加入的节点还有一种学习的状态（就是只围观不投票，这个是3.4以后的新版本引入的），为了解决那些需要很长时间才能完全同步进而影响系统提交的场景（主要是计算多数的时候把基数给拉大了）。
6. 相对于上面一次一个变更这种保守的方式还有一种joint consensus的做法，就是把新旧配置的变更当做一条请求产生日志并复制到其他节点，但此时还是旧配置的中节点才有选举权（其实就是过渡期的老队伍才是需要优先安抚的，保证即使出了问题这群人还能够按照老队伍来继续工作）。等到这个变更日志被多数成员使用后，所有的请求产生的日志要被复制到新旧配置的全部节点中，有点像是接力运动传接力棒的过程，传和接的人两个人都拿着接力棒跑。新配置会再次产生一条日志给全体成员，等到提交后不在新配置的节点自己就要主动退出了。接力的时候由于接棒的人速度还没有起来，所以需要传棒的人带着跑几步，等到速度起来了传棒的人就该放手休息了。


## 其他区别

除开概念上的一致性，通信模型上的差异导致功能上两者还是有很多区别的：

1. zab的任意节点都可以服务于客户端，数据可能过时，但一定是多数节点确认过的。raft先随机节点找到leader后就只跟leader玩了，leader压力山大。对时效性有要求的话zab可以主动使用sync来拿到最新的数据，这样可以平衡性能很时效性，对业务更加友好。后期zookepper甚至出现了observer节点，这种不投票但能干只读的活。哪有这么听话的百姓，没有任何权利还可以干点活（那不是一点，现实中大部分业务都是写少读多的）。
2. raft的事务是双向rpc，leader是要等follower干完活的结果的。这点性能肯定不如zab的广播，leader发广播后不管了，等follower异步完成后再广播通知leader结果。广播在及时性上还有点优势，就是leader确认结果后通知小弟提交的代价不一样。raft舍不得rpc的成本，就等到下一个心跳顺便通知提交，而zab确认后就直接广播给小弟提交了。
3. 对于已有集群加入新的节点时，raft相对简单，leader会发心跳过来。zookeeper还是推荐自己，等到收到现有节点的回复后，判断已经有leader了就放弃自己的投票。raft在新节点的加入过程中是不会阻塞对外请求的，原理是leader维护了所有follower的连续索引。目前zookeeper的zxid并不要求连续，确认新来的follower与leader的差异的时候需要锁定leader当前的数据。会阻塞对外请求。
4. raft只有follower会检查心跳（单向），一段时间收不到就一定会发起新一轮的投票。zookeeper的leader检查心跳的回复有没有过半数，follower检查是否有心跳（双向）。
5. 处理前任的数据方式有差异。zab在投票过程中通过广播不停的交换知识取得一致，而raft是先选出来再同步数据。所以zab对前任的数据是激进策略，只要现任知道就可以认为多数人已经知道，可以提交。raft只能先全都不认，由现任来提议再询问一次结果后才能够提交。对于前任的假死，两者差不多，靠epoch和term这种递增的计数器来分别。

## zookeeper吐槽和etcd的崛起

对网络的明感程度和底层存储模型的差异导致使用起来的体验差别也不小：

1. zab是基于多杈树的内存节点，是一个文件储存模型。实现的功能大部分通过临时节点和回调，感觉起来就像你在操作底层的API来实现一个高层级的应用功能。而且要注意各个节点的数据并不一致，客户端获取最新的数据必须自己主动同步。
2. zab的watch机制的回调是一次性的。如果更新频率高，需要重复注册而且不能确保全部更新都不漏。
3. zab这种瑞士军刀的体验感给开发者不小负担，用起来要小心翼翼。Curator这个有Netflix牛逼公司实现的开源zookeeper框架拯救了不少公司和程序员。话说开源的红利就是这个讨论，总有牛逼公司把一个理论弄出工业级的产品给业界使用。中国的互联网公司欠开源世界一大笔，只有少数几家在给世界级的开源项目做贡献。
4. zab一旦出现网络隔离导致选举（leader在少数区），选举过程中大量的数据交换导致选举比较慢，这段时间没有leader就无法对外提供服务。也许短时间的1秒跨机房网络隔离导致的不可用就会被放大到整个选举周期。就算leader在多数区，那少数区没有leader也是无法提供服务的，就算服务只需要使用少数区机房内部的资源也不行。因为zab自身的问题破坏了应用的可用性，这事说起来可严重了。
5. 服务缺乏扩展性，不用加机器来取得线性的服务能力。加机器只是可靠性增强，这做法很不互联网。这在互联网公司来看是不可接受的，万一公司多元化出现服务规模的持续暴涨，那垂直划分成多个小集群只能缓解，哪一天要水平联通的时候你都没地方哭。
6. 储存缺乏扩展性，节点最多能管1M的数据，这完全不能用来做分布式存储。小范围的kv集合只能小打小闹。
7. 服务的健康直接绑定在tcp长连接的活性上，这限制了应用对于服务活性的理解。tcp可用不代表服务可用。

有问题的东西google是不能忍的，基于raft的etcd解决了一些问题，抢回了zookeeper的不少市场，确实可以算作年轻的继任者了。raft的实现简单不少，以至于国内有不少玩家基于开源开码自己整出一些新的东西。在分布式系统中的数据分为控制数据和应用数据。使用etcd的场景默认处理的数据都是控制数据。对于应用数据，勉强来说只推荐数据量很小，但是更新访问频繁的情况。

1. 同样单leader的机制决定了跨机房的问题是解决不掉的。
2. go比java的性能强一点，也一样有GC问题，但这方面不要质疑google的能力，随着go的迭代，性能绝对会做的比zk好。
3. 客户端的使用restful API基于http，用起来简单多了，轻松跨语言。
4. 支持ssl，安全性这块还是得到重视。
5. 可扩展性，每个实例每秒支持一千次级别的写操作。

## 不同场景下的选择

zookeeper这么流行不是没有道理的，它有自己非常擅长的领域。在粗粒度分布式锁，分布式选主，主备高可用切换等不需要高 TPS 支持的场景下有不可替代的作用，而这些需求往往多集中在大数据、离线任务等相关的业务领域，因为大数据领域，讲究分割数据集，并且大部分时间分任务多进程 / 线程并行处理这些数据集，但是总是有一些点上需要将这些任务和进程统一协调，这时候就是 ZooKeeper 发挥巨大作用的用武之地。它可用做一个分布式领域协调的王者，但是服务发现注册中心之类的真的不合适。
