# 向Martin表以诚挚的敬意

第一次看ddia只到前面4章就有了这种向作者致敬的感觉，思维的高度确实相差几个数量级。上一次明显有这种感觉还是在看Jeff Dean的The Tail At Scale [link] https://ai.google/research/pubs/pub40801。 真诚的感谢行业前辈分享出如此高屋建瓴的文章，让从业者有有机会从更高的视角来看待行业。

## 数据系统的基石

1. 可靠性，可扩展性，可维护性。这个话题就不多讲感悟了，只说自己的认知。领域（非软件行业）里功能性需求所对应的翻译工作从来都是最简单的，领域专家配合高级点的程序员就可以做出满足需求的产品。非功能性需求才是难对付的，要实现好这些需求你需要的是软件行业里的领域专家。
2. 数据模型与查询语言。文档模型->关系模型->图形模型对应着数据间的联系紧密程度。
3. 存储与检索。有了模型后对应的存储在内存与外存的处理方式有很大差别，外存的引擎可以基于行（innodb）与列（LSM）来存储和检索，。
4. 编码与演化。为了走出程序（与进程外的世界交换数据），数据的流动就有了编码需求。常见的方式就是通过数据库(REDIS,MYSQL)，服务调用（REST,RPC），异步消息传递（kafka）。有了编解码，就要考虑数据前向和后向的兼容性。

## 分布式数据

1. 共享内存架构（shared-memory architecture）中，多处理器，内存和磁盘可以在同一个操作系统下相互连接，快速的相互连接允许任意处理器访问内存或磁盘的任意部分，所有的组件都属于一台单独的机器。这是大型机的时代和解决方案，问题在于成本增长速度远远快于性能的增长。（第一代早期的语言喜欢内存共享的方式来交换数据，就是一个大系统全部在一个进程里面。即使是现在这个年代，很多嵌入式系统都还是这样的只有一个进程。）
2. 共享磁盘架构（shared-disk architecture），它使用多台具有独立处理器和内存的机器，但将数据存储在机器之间共享的磁盘阵列上，这些磁盘通过快速网络连接。基本上这就是分布式的开端了，成本足以让一堆便宜的小型机来代替大型机。对磁盘的竞争还是会成为系统的瓶颈。（中间代的语言开始转向发送消息的方式来交换数据，即使在单个机器上也是这样。复杂系统开始被拆成多个进程了。）
3. 无共享架构（shared-nothing architecture）目前已经相当普及。在这种架构中运行数据库软件的每台机器/虚拟机都称为节点（node）。每个节点只使用各自的处理器，内存和磁盘。节点之间的任何协调，都是在软件层面使用传统网络实现的。网络基础设施才是无共享架构的基础。（现代语言基本上都是以发送消息的形式来交换数据，以至于发展出了大量的专用消息系统中间件来避免自己造轮子。各节点都只提供专一的服务，演化出了SOA再改名到微服务了。）
4. 分布式的数据底气就是冗余，通过空间成本来提高可靠性，主要有复制和分区两种做法。replication，这是早期的主流方案，就是各个节点全部保存一样的数据多份。mysql, redis基本都是这个套路。partition，这是目前的主流思路，把数据打散后尽量均匀丢给各个节点。两者概念是正交的，例如可以分区后再复制。

### 复制，replication

1. 可以让数据你用户距离更进提高访问速度，例如CDN。对于那些对时效性要求不高的只读请求（显示生活中大部分需求都是这样的），复制可以显著的提高吞吐量。目前有三种流行的变更复制算法：单领导者（single leader） ，多领导者（multi leader） 和无领导者（leaderless）。常见单领导者方案例如mysql，kafka都是一主多从，只有主可写，从随之接收主的变更通知。至于是同步还是异步，那是另一个正交的问题。同步影响性能，异步影响可靠性，混合使用能够取得一定的平衡（半同步）。重建或者添加新的从节点也是常规操作，需要快照加复制日志共同提供支持从追赶上主。
2. 单主复制
逻辑复制，基于语句的逻辑复制，需要消除不确定性，否则主从可能不一致。mysql的binlog就默认从statement升级到了基于行的row格式。row还有一个明显的好处是脱离了存储引擎的限制，可以轻易被外部系统使用。物理复制，哪些磁盘块中的哪些字节发生了更改就比较底层了。只有相同版本的引擎（至少数据的落地和加载是一致的）能够理解这种格式，例如postgresql。
故障切换。主出问题了需要剩下的多数从来选出新的主，即使旧主恢复了还得认可新主的地位，这貌似真的很民主。如果是异步复制就有数据延迟，是无法保证旧主的最新数据不丢失的。选举中要是出现两个势均力敌的候选者，还有可能出现各自为政的情况。
读自己的写，保证用户自己的输入已被正确保存并能够立刻看到。其他用户的更新可能稍等才会看到，这里面的道道很多，难以简述。简而言之就是用各种策略确认用户是否真的需要读主库。
单调读，可能会出现时光倒流后面的读取到更老的用户。用户的两次读取在不同的从上返回的结果是没有任何保证的。实现单调读取的一种方式是确保每个用户总是从同一个副本进行读取。
一致前缀读，如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。这个问题是分区特有的。一种解决方案是，确保任何因果相关的写入都写入相同的分区。
3. 多主复制
在单个数据中心内部使用多个主库很少是有意义的，因为好处很少超过复杂性的代价。redis的官方cluster集群方案就是多主，这个更多的是水平扩展瓜分了负载而不是主从关系。对于多活的数据中心还是有多主的需求的，特别是基于位置和基础设施的分布需要提供离用户更进的主节点。地大物博的祖国就是这样的，各大互联网公司都是分成几大区来部署多活数据中心的。其实国内的使用场景基本不会出现一个用户在两个数据中心的写冲突，所以问题不大。你要真赶上一个账号在各种设备渠道上天南地北几个地方同时操作一个东西，那一样处理不好。这种高难度的技术还是不要轻易使用的好，免得自己驾驭不了。（有点好奇wps的协同编辑文档是如何实现的，是基于时间片轮询的单主吗，那怎么解决写入冲突问题呢？还是说是基于内容的锁定的多主，虽然是多主但是并没有写冲突。）
4. 无主复制，这个去中心化的概念最近开始复苏，后事如何还得再观察。
5. 多副本仲裁。r和w被选为多数（超过 $n/2$ ）节点，因为这确保了$w + r> n$，同时仍然容忍多达$n/2$个节点故障。

### 分区，partition，sharding

1. 分区是一种有意将大型数据库分解成小型数据库的方式。即数据在不同节点上的副本，对于非常大的数据集或非常高的吞吐量，仅仅进行复制是不够的。分区的重点在于处理可扩展性，通过水平扩展的方式满足性能的近似线性增长。原理是平均理论，避免热点。
2. 老生常谈的按照范围或者散列做分区，例如数据库的分表就是第一个具体应用。基于主键的分区简单点而且有排序有事，就是次级索引用不了也可能会有热点效应。散列破坏了排序，到哪更加均匀。次级索引目前有两种主流方案，分别是基于文档和关键词。还是那句话，一维空间里折腾要么基于行（理论不那么优秀，但是用广泛），要么基于列。
3. 分区再平衡。举个具体例子mysql分表不够用了rehash，要做到之后是公平的，而且不能中断服务，尽快能够完成减少对系统的影响。hash%N算是一个反面教材，数据移动量那是相当的大。预分配足够多的分区然后打散到节点上比较靠谱，理想的实现是动态分区，分区的数量与数据量成正比。再平衡是一个相当消耗资源的过程，需要引起注意，尽量避免运维事故。
4. 分区后的服务发现。找到正确的节点处理请求大概有三种实现方式，一是任意节点都知道映射关系从而帮助请求跳转到正确的节点；二是有单独的路由维护映射信息，所有的请求都经过路由（codis）；三是客户端存储映射关系，直接访问需要的节点（redis cluster）。目前主流是使用zookeeper之类共识算法管理集群元数据，来帮助路由找到所需节点。

### 事务，transition

计算机网络中环节太多（就是间接层太多，相比传统行业来说工序太多出问题的概率就更大），为了缓解这个问题就有了事务这个概念。就是一堆操作要么成功，要么失败但对系统没有任何副作用，没有部分成功或者失败的说法。几十年前IBM发明第一个关系型数据库就引入事务了，后期的发展思路基本上没有太大变化。近来非关系型数据库的发展中就弱化了传统的事务概念，甚至重新定义或者放弃了事务概念。

1. 原子性，更好的术语是可中止性（abortability）。能够在出现错误时中止事务，丢弃该事务进行的所有写入变更的能力。（完全无关并发，也无关原子写）
2. 一致性，这个词用烂了，各种语境下都有特定的含义。这里一致性属于应用程序的属性，不属于数据库的属性。实际上是应用程序通过AID来实现C，确保系统一直都是正确的（满足既定约束）。C是目的，而AD是手段。可以考虑理解为及时性与完整性。违反及时性是为了追求最终一致性，而违反完整性就彻底不一致了。在大多数应用中，完整性比及时性重要得多。违反及时性可能令人困惑与讨厌，但违反完整性的结果可能是灾难性的。
3. 隔离性，多客户端的竞态。其实跟传统竞态一样需要锁来解决，锁的粒度跟性能成反比，就是数据库这块为了性能考虑到极致引入了多版本MVCC以及读写时间戳。锁的粒度从小到大为读未提交->读已提交->可重复读->串行化。INNODB的可重复读并没有自动检测单对象更新丢失的能力，这点不知道啥时候会考虑（应用程序是人写的，忘记上锁是一定会出现的）。而多对象的写入偏差问题更难发现，这时候大范围的锁或者序列化操作才能解决问题。一个事务中的写入改变另一个事务的搜索查询的结果被称为幻读，需要物化冲突的解决方案（就是给不存在的对象加锁，innodb的间隙锁）。串行化就像redis那样单线程排队，没有并发，当然是适合高速的RAM，硬盘还不能这样玩。
4. 持久性，就是落盘。没有完美的持久性，没人能够提供绝对的安全。
5. 存储过程，一个名声不太好但是使用的还不少的用来封装一组操作的工具。举例说一下redis里面lua经常用来干delete if equal之类的事情。
6. 2PL，两阶段锁定（完全不同于两阶段提交2PC），分别是共享锁和独占锁。间隙锁算是一个帝梓元消耗的谓词锁。

### 分布式的麻烦

任何可能出错的东西都会出错。只能挑重点梳理了，网络的问题以及时钟和时序问题。从不太可靠的潜在基础构建更可靠的系统是计算机领域的一个古老思想，这行业就是这个样子的。

1. 网络的不确定性。请求者者甚至不能分辨数据包是否被发送，唯一已知的是尚未收到任何响应，其他的都不能确认。被动超时是一个没得选的选择，但超时多久才能确认被请求的节点失效了呢？没有办法，异步的网络有太多种情况，延迟是没有上限的，太短的超时设定在一个抖动的尖峰就可能会出现误判进而加重系统的负担。异步的排队问题比看起来要难得多，整个链路上是有N个异步队列层层排队的。应用层在排队，到内核缓冲区又在排队，协议栈，交换机都在排队。而且商业化的多租户数据中心的目标就是提高利用率，很容易出现对共享资源的争抢而导致更加严重的抖动。
2. 时钟的不确定性。每台机器都有自己的时间概念，网络时间协议（NTP）可以用来纠正，而更精确的时间依赖于GPS。
3. 多分布式算法都依赖于法定人数，即在节点之间进行投票。决策需要来自多个节点的最小投票数，以减少对于某个特定节点的依赖

### 一致性与共识

分布式系统最重要的抽象之一就是共识（consensus），就是让所有的节点对某件事达成一致。线性一致性使目前看来最好理解的方案，对外表现得好像只有一个数据副本，而且所有的操作都是原子的。但事实上为了达到冗余的效果，还是有多副本的，只是线性一致的共识基本上是用在单主复制中。现代多核CPU上的内存甚至都不是线性一致的，除非使用了内存屏障（memory barrier）。

1. CP，如果应用需要线性一致性，且某些副本因为网络问题与其他副本断开连接，那么这些副本掉线时不能处理请求。请求必须等到网络问题解决，或直接返回错误。
2. AP，如果应用不需要线性一致性，那么某个副本即使与其他副本断开连接，也可以独立处理请求。

顺序，线性一致性和共识之间有着深刻的联系，远远超出软件这一特定领域。老实说线性一致性的大问题在于效率，速度还是太慢了，在有较大延迟的网络中性能低下。但是更快地线性一致算法不存在，只能放弃一些限制来换取更高的性能。弱化一点的关系例如因果一致性，需要知道哪个操作发生在哪个其他操作之前（happened before)。这里发展处多种技术，例如逻辑时钟，版本向量。工业界的实现可以称之为全序广播，要保证消息可靠交付（如果消息被传递到一个节点，它将被传递到所有节点）和相同顺序交付（消息以相同的顺序传递给每个节点），zookper和etcd都有实现。其实这种要求正是数据库复制所需要的，例如mysql的binlog就需要保证被发到所有从节点而且是与主节点一样的顺序来执行，这样就能保证主从一致。共识是如此之难，FLP假定确定性算法不能使用任何时钟或超时从而证明如果存在节点可能崩溃的风险，则不存在总是能够达成共识的算法。不过放宽一些限制后，发展出早期的2PC（跨多个节点的原子事务提交的算法，引入了协调者，准备阶段和提交阶段。例如数据库的XA事务）和成熟的zab（zookper使用）以及raft（etcd使用）算法。其实两阶段提交的弱点在于协调者，为了解决问题引入了一个新的问题。这是一个强模型，需要所有参与者都同意协调者才能做出同意的决定。只要有一个人反对或者没有反馈，整个过程都是阻塞的。后面的共识算法放宽了决定的条件，只是需要”未崩溃“的参与者同意即可的模型，而不是等待所有人。像ZooKeeper这样的工具为应用提供了“外包”的共识、故障检测和成员服务，基本上是分布式系统绕不开的需求，这个能容错的共识轮子想造的好太难了。

## 异构多数据系统

现实世界中的数据系统往往更为复杂。大型应用程序经常需要以多种方式访问和处理数据，没有一个数据库可以同时满足所有这些不同的需求。因此应用程序通常组合使用多种组件：数据存储，索引，缓存，分析系统，等等，并实现在这些组件中移动数据的机制。从高层次上看，存储和处理数据的系统可以分为两大类：源数据记录和衍生数据。数据源是权威的，衍生数据可以由数据源构建，多视角的衍生数据对于只读查询（大部分场景）的作用是至关重要的。简单举例如mysql记录数据和redis缓存数据。随着数据量级别的提升，出现了以mapReduce这样面向批处理，以及spark，flink这样面向流处理的方式。

服务（在线系统）
​服务等待客户的请求或指令到达。每收到一个，服务会试图尽快处理它，并发回一个响应。响应时间通常是服务性能的主要衡量指标，可用性通常非常重要（如果客户端无法访问服务，用户可能会收到错误消息）。交互式数据流动是符合人脑思考模式的，就是一根筋关注一个细节点并希望快点得到反馈。这里面各种等待和阻塞完全不符合CPU的思维模式。没办法，各司其职，努力盯住自己的一点东西要轻松点，分心关注太多事情比较累。这是典型的个人思维模式。

批处理系统（离线系统）
​一个批处理系统有大量的输入数据，跑一个作业（job） 来处理它，并生成一些输出数据，这往往需要一段时间（从几分钟到几天） ，所以通常不会有用户等待作业完成。相反，批量作业通常会定期运行（例如，每天一次） 。批处理作业的主要性能衡量标准通常是吞吐量（处理特定大小的输入所需的时间）。这个有点像各种缓冲区机制一样，先收集一批工作（缓冲区满）然后CPU一气呵成的运行完，给出一个最终结果。这个有点像将军层面的思考模式，手里有一堆兵等到毛贼积累到一定程度派出去一次性处理，即提高了出兵效率又保证了有结果。需要关注多个方面（人或者事情），最终以吞吐量（固定的任务所需的时间）为主要目标。这是典型的经理人的思维模式，贼像外包系统，一个模块的需求丢过去吧啦吧啦几天或者一周后给你个版本。或者定期出个周报，月报之类的结果给老板讲讲。有活要干，但是已经计划好了时间和预算，总会干的完，类似政府和国企的做法。

流处理系统（准实时系统）
​流处理介于在线和离线（批处理）之间，所以有时候被称为准实时（near-real-time）或准在线（nearline）处理。像批处理系统一样，流处理消费输入并产生输出（并不需要响应请求）。但是，流式作业在事件发生后不久就会对事件进行操作，而批处理作业则需等待固定的一组输入数据。这种差异使流处理系统比起批处理系统具有更低的延迟。这玩意就更像敏捷开发的职业经理人了做法了，看到啥需求就队伍先干出来给产品，不用凑个几天的工作量再去部署工作计划。可以随时弄个版本看最新的结果和继续接收新的需求。有活要干，也不知道手头的活干完之后还有多少活要干，总之就是一直干下去没有尽头，类似大部分民企的做法。

### 批处理

批处理的基石在于统一的接口，就像unix的管道以及web的URL。一个输入在某个系统产生的输出可以成为另一个系统的输入。在分布式的架构中HDFS相当于文件系统，而mapReduce相当于进程。mapper就是把自己的数据按照要求放入排序的表单，reducer是合并各个mapper输出的表单，类似归并的多机版本。这类操作基本上等于全表扫描，因为没有索引的概念。如果需要特定用户的数据，还是传统的索引更加高效。与Unix工具类似，MapReduce作业将逻辑与布线（配置输入和输出目录） 分离，这使得关注点分离，可以重用代码：一个团队可以实现一个专注做好一件事的作业；而其他团队可以决定何时何地运行这项作业。相比unix模型还是太复杂，MapReduce作业只有在前驱作业（生成其输入） 中的所有任务都完成时才能启动，而由Unix管道连接的进程会同时启动，输出一旦生成就会被消费。也就是说将算子的输出增量地传递给其他算子，不待输入完成便开始处理。而将中间状态存储在分布式文件系统中意味着这些文件被复制到多个节点，也是有开销的。这种物化的过程也不是完全没有好处，持久化使得容错变得容易了。​ 而Spark，Flink则采取了其他手段来避免这种物化。

### 流处理

一个事件由生产者producer也称为发布者或发送者生成一次，可能有多个消费者，相关的事件通常被聚合为一个主题。原则上将，文件或数据库就足以连接生产者和消费者：生产者将其生成的每个事件写入数据存储，且每个消费者定期轮询数据存储检查自上次运行以来新出现的事件。只是轮询周期和效率问题引出了流处理，看来计算机的哲学哪里都一样，消息系统kafka就是一种典型的流处理实现了。其实各种系统都有适合的领域，数据库可以提供长期的持久数据方便随机查询看中的是长期可用性，消息系统能够快速的通知变更看中的是时效性和变化，怎么用就看业务需要什么样的表现。

原则上消息是可以直接从生产者传递给消费者，类似UDP的broadcast。但是这种直连是非常脆弱的，容错能力有限。最后业界走向消息代理也就是消息队列的路上，其本质上是一种为处理消息流而优化的数据库系统。举例说明像mysql的binlog被canal，maxwell工具实时转发到redis或者kafka，是因为传统数据库在通知机制上能力有限，别指望数据库的触发器。其实按照以前的商业软件封闭式开发的行为，几乎是没有办法解析出日志中的信息在其他系统重放的。当开源成为一种方向后，这件事才变得容易实现，进而出现了这种捕捉变更的做法，以后估计会成为主流用法。

代理搞定了持久性之后，生产者只管确认代理收到了消息即可，至于何时能够到消费者那里就不是生产者关心的问题了。代理才需要确认消费者已经拿到消息了，典型的非阻塞异步的系统。至于整个传递工作正常完成后，消息还能够持久化多久那是另外的话题了。目前主流使用基于日志的消息系统过将消息追加到日志末尾来发送消息，而消费者通过依次读取日志来接收消息。为了扩展到比单个磁盘所能提供的更高吞吐量，对日志进行了分区托管到不同机器上。一个主题可以其实被拆成了携带相同类型消息的多个分区，对于分区里面的追加写是能够保证分区内消息的有序，不同分区的消息是没有有序性。通常情况下，当一个用户被指派了一个日志分区时，它会以简单的单线程方式顺序地读取单个分区中的消息，这种容易使用的模型基本上成为了主流。只要提高分区数就可以适应增加消费者，线性的性能提升用来提升并发度已经是比较好的结果了。顺序消费一个分区使得确认消息是否被消费变得简单，代理不需要确认每条消息，只需要定期记录一下消费者的确认即可。等于批量确认，这也有一点风险，就是消费者失效后可能会把还未确认的消息再次发给其他消费者造成重复消费。

由于流处理的时效性不错，常被用于监控或者警报。举例视频监控流就能够提供预警，或者出现了不满足时间约束，地点约束等等各种被看成风控的事件。这个模型在以前防火主机中应用的非常典型，整个系统就是建立规则，事件监测，响应命令执行。再者流处理还可以用于统计分析，例如配合bloom filter检测有效性，使用loglog估算业务近似指标。如果谈到精确的时间，流处理还是有事件时间与处理时间这两个概念的。请求发生的时间与处理的时间是两回事，完全是两个不同的系统。这在批处理是显而易见的，流处理中弱化了两者的时间差异。

在容错方面流处理就不简单了，不像批处理那样确定的工作有确定的结果而且还能够做到刚好处理一次。等待某个任务完成之后再使其输出可见并不是一个可行选项，因为你永远无法处理完一个无限的流。在这个背景下就只能分批次来权衡了，按照时间或者事件数量，这在各种树落地策略中都广泛使用。例如redis的AOF选择每次或者每秒钟后落地。这也仅限于流处理的上下文范畴，在一个小的批次层面上提供恰好一次的语义，如果有任何其他衍生的副作用例如发个邮件之类的就无法避免重复发送。最好的办法还是尽量使用幂等性来避免副作用。

流处理与目前流行的微服务架构看起来都是一种解耦手段，但底层理念还是有典型的区别的。微服务基本上使用同步的请求交互式风格，这只有系统足够内聚不用经常访问外部数据的时候才行得通。流处理底层是单向的异步消息，就是通知。可以想象一下外汇交易市场的订单服务，微服务的典型使用方式是去汇率服务查询当前汇率再生成订单，每一笔订单都会在内部多走2步网络调用了。如果是订单服务订阅汇率数据的变更并，就可以直接在服务内部生成订单了。在订单数量级别很高的时候，流处理显然更加高效。总的来说请求响应的方式已经根深蒂固的散布到各个领域，包括各种库，数据库，框架。但发展在软件行业还是快速的，基于订阅和变更的系统开始多起来。从http到websocket就是web行业的变更。

## 数据系统的未来

衍生数据集是写路径和读路径相遇的地方。例如mysql数据库的二级索引，写入的时候更新衍生数据就是为了后面按照二级索引的查询数据做准备。简单来说这里的索引就是一种平衡写入与读取的工作量。举个更加极端的例子，就是普通用户发微博的时候写入其实已经为博客的关注者准备好了数据（缓存），等博客的关注者来读取的时候就可以不用查询了。而对于大V的名人关注者众多的情况，写入就没有缓存，读取数据还是根据索引查询出来的。依照业务场景的数据量级别来用不同的衍生数据来确定读写的边界是数据模型的魅力所在。

到最后还是要考虑到行业道德的问题。​然而随着算法决策变得越来越普遍，被某种算法（准确地或错误地）标记为有风险的某人可能会遭受大量这种“No”的决定。系统性地被排除在工作，航旅，保险，租赁，金融服务，以及其他社会关键领域之外。这是一种对个体自由的极大约束，因此被称为“算法监狱” 。举例说明如果真的在国内全面启用征信系统，势必会对没有认识到问题严重性而有不良征信的公民造成不轻的打击。在尊重人权的国家，刑事司法系统会做无罪推定。但反过来讲一旦自动化系统可以系统地，任意地将一个人排除在社会参与之外，那不需要任何有罪的证明，而且一定时间内几乎没有申诉的机会。当一辆自动驾驶汽车引发事故时，谁来负责？如果机器学习系统的决定要受到司法审查，如何向法官解释算法是如何做出决定的吗？
